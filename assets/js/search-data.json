{
  
    
        "post0": {
            "title": "Movie Lens, IMDB and Latent Factors",
            "content": "%matplotlib inline %reload_ext autoreload !pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastai import * from fastai.text.all import * . Download the FastAI IMDB entries . path=untar_data(URLs.IMDB) #/root/.fastai/data/imdb get_imdb = partial(get_text_files,folders=[&#39;train&#39;,&#39;test&#39;,&#39;unsup&#39;]) . Build a datablock . dls_lm = DataBlock( blocks = TextBlock.from_folder(path,is_lm=True), get_items=get_imdb,splitter=RandomSplitter(0.1) ).dataloaders(path,path=path,bs=128,seq_len=80) . Save the Datablock for later when we require the word-token mapping when we use the encoder . import pickle pickle.dump( dls_lm , open( &quot;savelm3.p&quot;, &quot;wb&quot; ) ) !cp /content/savelm3.p /content/gdrive/MyDrive . Create the learner . learnlm = language_model_learner(dls_lm,AWD_LSTM,drop_mult=0.3,metrics=[accuracy, Perplexity()]).to_fp16() . Fit the learner and optionally save the model . learnlm.fit_one_cycle(1,2e-2) learnlm.save(&#39;1epoch&#39;) !cp /root/.fastai/data/imdb/models/1epoch.pth /content/gdrive/MyDrive . epoch train_loss valid_loss accuracy perplexity time . 0 4.015330 3.898061 0.301384 49.306740 26:15 . Note it uses a model subfolder within the path . print(path) . /root/.fastai/data/imdb . Unfreeze the model and fit. Note this took over five hours. Save the encoder for later . learnlm.unfreeze() learnlm.fit_one_cycle(10,2e-3) # 5 hours learnlm.save_encoder(&#39;finetunedF&#39;) !cp /root/.fastai/data/imdb/models/finetunedF.pth /content/gdrive/MyDrive #learnlm.load_encoder(&#39;finetunedF&#39;) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.763633 | 3.761119 | 0.317067 | 42.996529 | 31:10 | . 1 | 3.701758 | 3.701771 | 0.323646 | 40.518990 | 30:32 | . 2 | 3.633814 | 3.649657 | 0.329085 | 38.461483 | 30:27 | . 3 | 3.539764 | 3.616908 | 0.332909 | 37.222294 | 30:55 | . 4 | 3.499323 | 3.596352 | 0.335529 | 36.464954 | 30:44 | . 5 | 3.433929 | 3.583806 | 0.337819 | 36.010345 | 30:32 | . 6 | 3.377377 | 3.572593 | 0.339740 | 35.608814 | 30:54 | . 7 | 3.294219 | 3.568537 | 0.340728 | 35.464653 | 30:20 | . 8 | 3.233820 | 3.572530 | 0.341071 | 35.606556 | 30:22 | . 9 | 3.214494 | 3.576734 | 0.340802 | 35.756580 | 30:31 | . &lt;fastai.text.learner.LMLearner at 0x7fcc65a875d0&gt; .",
            "url": "https://conwyn.github.io/MyBlogs/fastai/language/encoder/2021/05/22/LearnIMDBLM.html",
            "relUrl": "/fastai/language/encoder/2021/05/22/LearnIMDBLM.html",
            "date": " • May 22, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Playing with CKKS",
            "content": "CKSS is a method of taking Complex (so Real) numbers and generating polynomials and then encrypting those polynomials. It is possible to add and multiply these encrypted polynomials and then decrypt them giving the answers as if the same operation applied to the original polynomials and the actual initial Real numbers. So ideal for deep learning matrix multipliction. . from numpy.polynomial import Polynomial import numpy as np def round_coordinates(coordinates): &quot;&quot;&quot;Gives the integral rest.&quot;&quot;&quot; coordinates = coordinates - np.floor(coordinates) return coordinates def coordinate_wise_random_rounding(coordinates): &quot;&quot;&quot;Rounds coordinates randonmly.&quot;&quot;&quot; r = round_coordinates(coordinates) f = np.array([np.random.choice([c, c-1], 1, p=[1-c, c]) for c in r]).reshape(-1) rounded_coordinates = coordinates - f rounded_coordinates = [int(coeff) for coeff in rounded_coordinates] return rounded_coordinates class CKKSEncoder: &quot;&quot;&quot;Basic CKKS encoder to encode complex vectors into polynomials.&quot;&quot;&quot; def __init__(self, M:int, scale:float): &quot;&quot;&quot;Initializes with scale.&quot;&quot;&quot; self.xi = np.exp(2 * np.pi * 1j / M) self.M = M self.create_sigma_R_basis() self.scale = scale @staticmethod def vandermonde(xi: np.complex128, M: int) -&gt; np.array: &quot;&quot;&quot;Computes the Vandermonde matrix from a m-th root of unity.&quot;&quot;&quot; N = M //2 matrix = [] # We will generate each row of the matrix for i in range(N): # For each row we select a different root root = xi ** (2 * i + 1) row = [] # Then we store its powers for j in range(N): row.append(root ** j) matrix.append(row) return matrix def sigma_inverse(self, b: np.array) -&gt; Polynomial: &quot;&quot;&quot;Encodes the vector b in a polynomial using an M-th root of unity.&quot;&quot;&quot; # First we create the Vandermonde matrix A = CKKSEncoder.vandermonde(self.xi, M) # Then we solve the system coeffs = np.linalg.solve(A, b) # Finally we output the polynomial p = Polynomial(coeffs) return p def sigma(self, p: Polynomial) -&gt; np.array: &quot;&quot;&quot;Decodes a polynomial by applying it to the M-th roots of unity.&quot;&quot;&quot; outputs = [] N = self.M //2 # We simply apply the polynomial on the roots for i in range(N): root = self.xi ** (2 * i + 1) output = p(root) outputs.append(output) return np.array(outputs) def pi(self, z: np.array) -&gt; np.array: &quot;&quot;&quot;Projects a vector of H into C^{N/2}.&quot;&quot;&quot; N = self.M // 4 return z[:N] def pi_inverse(self, z: np.array) -&gt; np.array: &quot;&quot;&quot;Expands a vector of C^{N/2} by expanding it with its complex conjugate.&quot;&quot;&quot; z_conjugate = z[::-1] z_conjugate = [np.conjugate(x) for x in z_conjugate] return np.concatenate([z, z_conjugate]) def create_sigma_R_basis(self): &quot;&quot;&quot;Creates the basis (sigma(1), sigma(X), ..., sigma(X** N-1)).&quot;&quot;&quot; self.sigma_R_basis = np.array(self.vandermonde(self.xi, self.M)).T def compute_basis_coordinates(self, z): &quot;&quot;&quot;Computes the coordinates of a vector with respect to the orthogonal lattice basis.&quot;&quot;&quot; output = np.array([np.real(np.vdot(z, b) / np.vdot(b,b)) for b in self.sigma_R_basis]) return output def sigma_R_discretization(self, z): &quot;&quot;&quot;Projects a vector on the lattice using coordinate wise random rounding.&quot;&quot;&quot; coordinates = self.compute_basis_coordinates(z) rounded_coordinates = coordinate_wise_random_rounding(coordinates) y = np.matmul(self.sigma_R_basis.T, rounded_coordinates) return y def encode(self, z: np.array) -&gt; Polynomial: &quot;&quot;&quot;Encodes a vector by expanding it first to H, scale it, project it on the lattice of sigma(R), and performs sigma inverse. &quot;&quot;&quot; pi_z = self.pi_inverse(z) scaled_pi_z = self.scale * pi_z rounded_scale_pi_zi = self.sigma_R_discretization(scaled_pi_z) p = self.sigma_inverse(rounded_scale_pi_zi) # We round it afterwards due to numerical imprecision coef = np.round(np.real(p.coef)).astype(int) p = Polynomial(coef) return p def decode(self, p: Polynomial) -&gt; np.array: &quot;&quot;&quot;Decodes a polynomial by removing the scale, evaluating on the roots, and project it on C^(N/2)&quot;&quot;&quot; rescaled_p = p / self.scale z = self.sigma(rescaled_p) pi_z = self.pi(z) return pi_z M = 8 N = M //2 scale = 64 # We set xi, which will be used in our computations xi = np.exp(2 * np.pi * 1j / M) xi encoder = CKKSEncoder(M,scale) . M = 8 N = M //2 # We set xi, which will be used in our computations xi = np.exp(2 * np.pi * 1j / M) xi . (0.7071067811865476+0.7071067811865475j) . b = np.array([1, 2, 3, 4]) b p = encoder.sigma_inverse(b) p b_reconstructed = encoder.sigma(p) b_reconstructed np.linalg.norm(b_reconstructed - b) . 6.944442800358888e-16 . z = np.array([3 +4j, 2 - 1j]) z p = encoder.encode(z) print(p) encoder.decode(p) . poly([160. 90. 160. 45.]) . array([2.99718446+3.99155337j, 2.00281554-1.00844663j]) . z = np.array([1,0]) z p = encoder.encode(z) print(p) encoder.decode(p) . poly([ 32. 22. 0. -23.]) . array([0.99718446-0.01104854j, 0.00281554-0.01104854j]) . z = np.array([2,0]) z p = encoder.encode(z) print(p) encoder.decode(p) . poly([ 64. 46. 0. -46.]) . array([ 2.016466+0.0000000e+00j, -0.016466+4.4408921e-16j]) . z = np.array([3,0]) z p = encoder.encode(z) print(p) encoder.decode(p) . poly([ 96. 68. 0. -67.]) . array([2.99155337+0.01104854j, 0.00844663+0.01104854j]) . z = np.array([4,0]) z p = encoder.encode(z) print(p) encoder.decode(p) . poly([128. 90. 0. -91.]) . array([3.99978637e+00-0.01104854j, 2.13634457e-04-0.01104854j]) . z = np.array([5,0]) z p = encoder.encode(z) print(p) encoder.decode(p) . poly([ 160. 113. 0. -113.]) . array([4.99697082e+00-4.44089210e-16j, 3.02917894e-03+1.33226763e-15j]) . z = np.array([1,0]) z p = encoder.encode(z) print(p) encoder.decode(p) . poly([ 32. 23. 0. -23.]) . array([ 1.008233+0.00000000e+00j, -0.008233+2.22044605e-16j]) . a1=Polynomial([32,22,0,-23]) print(a1) a2 = 2 * a1 print(a2) encoder.decode(a2) . poly([ 32. 22. 0. -23.]) poly([ 64. 44. 0. -46.]) . array([1.99436891-0.02209709j, 0.00563109-0.02209709j]) . import numpy as np from numpy.polynomial import polynomial as poly def polymul(x, y, modulus, poly_mod): &quot;&quot;&quot;Multiply two polynoms Args: x, y: two polynoms to be multiplied. modulus: coefficient modulus. poly_mod: polynomial modulus. Returns: A polynomial in Z_modulus[X]/(poly_mod). &quot;&quot;&quot; return np.int64( np.round(poly.polydiv(poly.polymul(x, y) % modulus, poly_mod)[1] % modulus) ) def polyadd(x, y, modulus, poly_mod): &quot;&quot;&quot;Add two polynoms Args: x, y: two polynoms to be added. modulus: coefficient modulus. poly_mod: polynomial modulus. Returns: A polynomial in Z_modulus[X]/(poly_mod). &quot;&quot;&quot; return np.int64( np.round(poly.polydiv(poly.polyadd(x, y) % modulus, poly_mod)[1] % modulus) ) . def gen_binary_poly(size): &quot;&quot;&quot;Generates a polynomial with coeffecients in [0, 1] Args: size: number of coeffcients, size-1 being the degree of the polynomial. Returns: array of coefficients with the coeff[i] being the coeff of x ^ i. &quot;&quot;&quot; return np.random.randint(0, 2, size, dtype=np.int64) def gen_uniform_poly(size, modulus): &quot;&quot;&quot;Generates a polynomial with coeffecients being integers in Z_modulus Args: size: number of coeffcients, size-1 being the degree of the polynomial. Returns: array of coefficients with the coeff[i] being the coeff of x ^ i. &quot;&quot;&quot; return np.random.randint(0, modulus, size, dtype=np.int64) def gen_normal_poly(size): &quot;&quot;&quot;Generates a polynomial with coeffecients in a normal distribution of mean 0 and a standard deviation of 2, then discretize it. Args: size: number of coeffcients, size-1 being the degree of the polynomial. Returns: array of coefficients with the coeff[i] being the coeff of x ^ i. &quot;&quot;&quot; return np.int64(np.random.normal(0, 2, size=size)) . def keygen(size, modulus, poly_mod): &quot;&quot;&quot;Generate a public and secret keys Args: size: size of the polynoms for the public and secret keys. modulus: coefficient modulus. poly_mod: polynomial modulus. Returns: Public and secret key. sk = gen_binary_poly(size) a = gen_uniform_poly(size, modulus) e = gen_normal_poly(size) &quot;&quot;&quot; b = polyadd(polymul(-a, sk, modulus, poly_mod), -e, modulus, poly_mod) return (b, a), sk . def encrypt(pk, size, q, t, poly_mod, pt): &quot;&quot;&quot;Encrypt an integer. Args: pk: public-key. size: size of polynomials. q: ciphertext modulus. t: plaintext modulus. poly_mod: polynomial modulus. pt: integer to be encrypted. Returns: Tuple representing a ciphertext. &quot;&quot;&quot; &quot;&quot;&quot; e1 = gen_normal_poly(size) e2 = gen_normal_poly(size) u = gen_binary_poly(size) &quot;&quot;&quot; # encode the integer into a plaintext polynomial # m = np.array([pt] + [0] * (size - 1), dtype=np.int64) % t m = pt %t delta = q // t scaled_m = m * delta % q ct0 = polyadd( polyadd( polymul(pk[0], u, q, poly_mod), e1, q, poly_mod), scaled_m, q, poly_mod ) ct1 = polyadd( polymul(pk[1], u, q, poly_mod), e2, q, poly_mod ) return (ct0, ct1) . def decrypt(sk, size, q, t, poly_mod, ct): &quot;&quot;&quot;Decrypt a ciphertext Args: sk: secret-key. size: size of polynomials. q: ciphertext modulus. t: plaintext modulus. poly_mod: polynomial modulus. ct: ciphertext. Returns: Integer representing the plaintext.# &quot;&quot;&quot; scaled_pt = polyadd( polymul(ct[1], sk, q, poly_mod), ct[0], q, poly_mod ) decrypted_poly = np.round(scaled_pt * t / q) % t # return int(decrypted_poly[0]) return (decrypted_poly) . size=2**2 e1 = gen_normal_poly(size) e2 = gen_normal_poly(size) u = gen_binary_poly(size) # polynomial modulus degree n = 2**2 # ciphertext modulus q = 2**15 # plaintext modulus t = 2**8 # polynomial modulus poly_mod = np.array([1] + [0] * (n - 1) + [1]) # Keygen sk = gen_binary_poly(size) modulus = 2**15 a = gen_uniform_poly(size, modulus) e = gen_normal_poly(size) pk, sk = keygen(n, q, poly_mod) . ct1 = encrypt(pk, n, q, t, poly_mod, np.array([ 32,22, 0, -23])) print(ct1) decrypted_ct1 = decrypt(sk, n, q, t, poly_mod, ct1) print(decrypted_ct1) . (array([29281, 29854, 2994, 15207]), array([23343, 7572, 16473, 1144])) [ 32. 22. 0. 233.] . ct2= (ct1[0] + ct1[0],ct1[0]+ct1[0]) print(ct2) decrypted_ct2 = decrypt(sk, n, q, t, poly_mod, ct2) print(decrypted_ct2) . (array([58562, 59708, 5988, 30414]), array([58562, 59708, 5988, 30414])) [219. 128. 221. 184.] . def gen_binary_poly(size): &quot;&quot;&quot;Generates a polynomial with coeffecients in [0, 1] Args: size: number of coeffcients, size-1 being the degree of the polynomial. Returns: array of coefficients with the coeff[i] being the coeff of x ^ i. &quot;&quot;&quot; return np.random.randint(0, 2, size, dtype=np.int64) def gen_uniform_poly(size, modulus): &quot;&quot;&quot;Generates a polynomial with coeffecients being integers in Z_modulus Args: size: number of coeffcients, size-1 being the degree of the polynomial. Returns: array of coefficients with the coeff[i] being the coeff of x ^ i. &quot;&quot;&quot; return np.random.randint(0, modulus, size, dtype=np.int64) def gen_normal_poly(size): &quot;&quot;&quot;Generates a polynomial with coeffecients in a normal distribution of mean 0 and a standard deviation of 2, then discretize it. Args: size: number of coeffcients, size-1 being the degree of the polynomial. Returns: array of coefficients with the coeff[i] being the coeff of x ^ i. &quot;&quot;&quot; . np.int64(np.random.normal(0, 2, size=5)) . array([ 0, 0, 2, -1, -2]) . import math np.random.normal(0,math.sqrt(2),5) . array([-0.15210849, -1.40642172, -0.50763401, -0.31183585, 1.27311431]) . an_array = np.array([1, 2, 3]) def double(x): return x * 2 mapped_array = double(an_array) print(mapped_array) . [2 4 6] . from numpy.polynomial import polynomial as poly import numpy as np import math # Create a function that adds 100 to something add_100 = lambda i: i + 100 # Create a vectorized function def randomZO(x) : lookup = [-1,0,0,1] return lookup[x] #given p=0.5 create q = 1/p so 2 create array 2q with middle elements 0 and 0 otherwise -1 and 1. change rndint to be 0,2q #So p = 0.25 q = 4 array 11100111 def randomZOinverse(x) : if x == 0 : return 0 return 1 qmod = 2**10 P = 2**8 poly_mod = np.array([1,0,0,0,1]) def ZOgenerator( ): vectorized_ZO = np.vectorize(randomZO) vectorized_ZOinverse = np.vectorize(randomZOinverse) sa = vectorized_ZO(np.random.randint(0,4,4)) sai = vectorized_ZOinverse(sa) while not((2 == (sai == 1).sum()) ) : sa = vectorized_ZO(np.random.randint(0,4,4)) sai = vectorized_ZOinverse(sa) return sa s = ZOgenerator() sk = (1,s) # ea = np.random.normal(0,math.sqrt(2),4) e = (ea) qmodsmall = 5 aa = np.random.randint(0,qmodsmall,4) a = (aa) b1 = poly.polymul(a,s) b2q,b2r = poly.polydiv(b1%qmod,poly_mod) #print(&quot;b2r&quot;) #print(b2r) b3 = poly.polyadd(e,-b2r%qmod) b4q,b4r = poly.polydiv(b3%qmod,poly_mod) b = b4r pk = (b,a) #validation key PQ = P*qmod av = np.random.randint(0,qmodsmall,4) ev = np.random.normal(0,math.sqrt(2),4) b1 = poly.polymul(av,s) b2q,b2r = poly.polydiv(b1%PQ,poly_mod) #print(&quot;b2r&quot;) #print(b2r) b3 = poly.polyadd(e,-b2r%PQ) b4q,b4r = poly.polydiv(b3%PQ,poly_mod) Ps2 = P*(poly.polymul(s,s)) b5 = poly.polyadd(b3,Ps2) #br4 b6q,b6r = poly.polydiv(b5%PQ,poly_mod) bv = b6r evk = (bv,av) #encrypt def ckksencrypt(m): v = ZOgenerator() e0 = np.random.normal(0,math.sqrt(2),4) e1 = np.random.normal(0,math.sqrt(2),4) c0a = poly.polymul(v,b) c0b = poly.polyadd(c0a,e0) c0c = poly.polyadd(c0b,m) c0q,c0r = poly.polydiv(c0c%qmod,poly_mod) c0 = c0r%qmod c1a = poly.polymul(v,a) c1b = poly.polyadd(c1a,e1) c1q,c1r = poly.polydiv(c1b%qmod,poly_mod) c1 = c1r%qmod return (c0,c1) #decrypt def ckksdecrypt(c0,c1,s) : dq,dr=poly.polydiv(poly.polyadd(c0,poly.polymul(c1,s))%qmod,poly_mod) dm = dr % qmod #print(&quot;s a b c0 c1 dm&quot;) #print(s,a,b) #print(c0,c1) #print(dm) return (np.around(dm)) # m0 = np.array([30,40,50,60]) m2 = np.array([60,50,40,30]) gc0,gc1 = ckksencrypt(m0) gc2,gc3 = ckksencrypt(m2) gs = s gc4 = np.add(gc0,gc2) gc5 = np.add(gc1,gc3) print(ckksdecrypt(gc0,gc1,gs)) print(ckksdecrypt(gc2,gc3,gs)) print(ckksdecrypt(gc4,gc5,gs)) d0m = poly.polymul(gc0,gc2) d0q,d0r = poly.polydiv(d0m,poly_mod) d0 = d0r d1ma = poly.polymul(gc0,gc3) d1mb = poly.polymul(gc1,gc2) d1m =poly.polyadd(d1ma,d1mb) d1q,d1r = poly.polydiv(d1m,poly_mod) d1 = d1r d2m = poly.polymul(gc1,gc3) d2q,d2r = poly.polydiv(d2m,poly_mod) d2 = d2r print(d0,d1,d2) print (&quot;evk&quot;) print (evk) d2evk0 = poly.polymul(d2,bv)%qmod d2evk1 = poly.polymul(d2,av)%qmod d2evk0q,d2evk0r = (poly.polydiv(d2evk0,poly_mod)) d2evk1q,d2evk1r = (poly.polydiv(d2evk1,poly_mod)) print(&quot;d2ev&quot;) print(d2evk0r,d2evk1r) d2a0 = poly.polyadd(d0,d2evk0r/P) d2a1 = poly.polyadd(d1,d2evk1r/P) d2a0q,d2a0r = poly.polydiv(d2a0%qmod,poly_mod) d2a1q,d2a1r = poly.polydiv(d2a1%qmod,poly_mod) gc6 =d2a0r gc7 = d2a1r print(&quot;decrypt mult&quot;) print(ckksdecrypt(gc6,gc7,gs)) . [27. 38. 47. 62.] [59. 52. 35. 33.] [85. 90. 83. 95.] [-4256.04489788 119.78987288 4447.73982358 8308.64367718] [-64520.64643198 24433.53648975 125827.70150041 143102.38467171] [-7066.59732099 4688.89450294 15407.15132969 15943.79869264] evk (array([ 2.61631816e+05, 2.62142369e+05, -1.98992018e+00, 2.62143947e+05]), array([1, 1, 0, 0])) d2ev [-225.80957222 -931.77543116 16.27155685 244.74714661] [-482.39601362 694.29718196 640.04583264 630.95002233] decrypt mult [ 234. 227. 467. 1009.] .",
            "url": "https://conwyn.github.io/MyBlogs/fastai/fhe/ckks/2021/05/22/CKKS.html",
            "relUrl": "/fastai/fhe/ckks/2021/05/22/CKKS.html",
            "date": " • May 22, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Inserting Multicategories into Reviews",
            "content": "%matplotlib inline %reload_ext autoreload !pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastai import * !pip install git+https://github.com/alberanid/imdbpy . |████████████████████████████████| 727kB 4.3MB/s |████████████████████████████████| 51kB 7.4MB/s |████████████████████████████████| 204kB 40.0MB/s |████████████████████████████████| 1.2MB 38.2MB/s |████████████████████████████████| 51kB 7.9MB/s |████████████████████████████████| 61kB 8.3MB/s Mounted at /content/gdrive Collecting git+https://github.com/alberanid/imdbpy Cloning https://github.com/alberanid/imdbpy to /tmp/pip-req-build-hmq9dz76 Running command git clone -q https://github.com/alberanid/imdbpy /tmp/pip-req-build-hmq9dz76 Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.7/dist-packages (from IMDbPY==2021.5.12) (1.4.15) Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from IMDbPY==2021.5.12) (4.2.6) Requirement already satisfied: importlib-metadata; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy-&gt;IMDbPY==2021.5.12) (4.0.1) Requirement already satisfied: greenlet!=0.4.17; python_version &gt;= &#34;3&#34; in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy-&gt;IMDbPY==2021.5.12) (1.1.0) Requirement already satisfied: typing-extensions&gt;=3.6.4; python_version &lt; &#34;3.8&#34; in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;SQLAlchemy-&gt;IMDbPY==2021.5.12) (3.7.4.3) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version &lt; &#34;3.8&#34;-&gt;SQLAlchemy-&gt;IMDbPY==2021.5.12) (3.4.1) Building wheels for collected packages: IMDbPY Building wheel for IMDbPY (setup.py) ... done Created wheel for IMDbPY: filename=IMDbPY-2021.5.12-cp37-none-any.whl size=299286 sha256=d58b37342e012a2b2db7958a97fe1fb458967157d5017d9fad86eef58458ebc9 Stored in directory: /tmp/pip-ephem-wheel-cache-63933vf_/wheels/0f/09/61/190df5e0276765680540f1562f2abca80e725a7e48595e993f Successfully built IMDbPY Installing collected packages: IMDbPY Successfully installed IMDbPY-2021.5.12 . Define the number of higher weights to investigate (FACTORS) and the number of heaviest films (Twenty). . from fastai.collab import * from fastai.tabular.all import * FACTORS=5 #Take the 10 latent factors Twenty = 100 #Only analyse 100 films # We need to bring in a prepared WiKi IMDB language model encoder and its vocab. import pickle import collections from imdb import IMDb . COLAB has no data retained between sessions so data is stored on your Google drive. The encoder is expected to be found in a folder called models . !mkdir /content/models . !cp /content/gdrive/MyDrive/finetunedE.pth /content/models . !cp /content/gdrive/MyDrive/savelm.p /content dls_lm = pickle.load( open( &quot;/content/savelm.p&quot;, &quot;rb&quot; ) ) . From the book. Get Movie Lens and identify 50 Latent Factors . path = untar_data(URLs.ML_100k) path.ls() ratings = pd.read_csv(path/&#39;u.data&#39;,delimiter=&#39; t&#39;,header=None,names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) ratings.head() movies = pd.read_csv(path/&#39;u.item&#39;,delimiter=&#39;|&#39;,encoding=&#39;latin-1&#39;,usecols=(0,1),names=(&#39;movie&#39;,&#39;title&#39;),header=None) movies.head() ratings=ratings.merge(movies) ratings.head() dls = CollabDataLoaders.from_df(ratings,item_name=&#39;title&#39;,bs=64) dls.show_batch() . user title rating . 0 542 | My Left Foot (1989) | 4 | . 1 422 | Event Horizon (1997) | 3 | . 2 311 | African Queen, The (1951) | 4 | . 3 595 | Face/Off (1997) | 4 | . 4 617 | Evil Dead II (1987) | 1 | . 5 158 | Jurassic Park (1993) | 5 | . 6 836 | Chasing Amy (1997) | 3 | . 7 474 | Emma (1996) | 3 | . 8 466 | Jackie Chan&#39;s First Strike (1996) | 3 | . 9 554 | Scream (1996) | 3 | . learn = collab_learner(dls, n_factors=50, y_range = (0,5.5)) learn.fit_one_cycle(5,5e-3,wd=0.1) . epoch train_loss valid_loss time . 0 | 0.959085 | 0.955102 | 00:07 | . 1 | 0.869222 | 0.876647 | 00:07 | . 2 | 0.745436 | 0.841352 | 00:07 | . 3 | 0.596926 | 0.825848 | 00:07 | . 4 | 0.482670 | 0.826195 | 00:07 | . Now look at the latent weights for each movie and pick the highest (FACTORS) weights and then using the movie title find the IMDB reviews later . movie_weight = learn.model.i_weight.weight idxs = movie_weight.argsort(descending=True) idxs5 = idxs[:,0:FACTORS] # Originally 5 movie_weight5 = [movie_weight[i,idxs5[i]] for i in range(movie_weight.shape[0])] idxm5s = movie_weight[:,0].argsort(descending=True) movie_weight5top = [] for i in range(FACTORS): #idxm5s: movie_weight5top.append(movie_weight5[idxm5s[i]]) movie_weight5toptitles = [] for i in range(FACTORS): #idxm5s: T0 = idxm5s[i].item() T1 = movies.iloc[T0].title movie_weight5toptitles.append(T1) newidxs=idxs #now give the review the attribute;idxs contains the latency number we just need to idxm5scpu to pull the high line and generate Lx Ly Lz assoiated with text idxs5[0:FACTORS,0:FACTORS] . idxs5list=[] for iii in range(idxs5.shape[0]): for jjj in range(idxs5.shape[1]): idxs5list.append(idxs5[iii][jjj].item()) counter=collections.Counter(idxs5list) print(counter.most_common(20)) . [(3, 318), (15, 298), (12, 272), (42, 258), (37, 254), (13, 252), (31, 247), (22, 244), (10, 231), (4, 224), (41, 218), (2, 212), (0, 211), (29, 209), (27, 209), (39, 203), (7, 196), (34, 184), (35, 182), (16, 182)] . Latency=idxs[idxm5s[:],0:FACTORS] . I notice that Latent Factor zero was dominating the analysis so here I remove it. . def tensorvaluezero(x): sum = False for i in x: e = i.item() if (e == 0.0): sum=True return sum TCList = [] for i,v in enumerate(Latency): if (tensorvaluezero(v.cpu()) == False ) : TCList.append(i) . len(TCList),len(Latency) . -211 . Just an idea but try find films with similar root mean square of ordinality. . # def tensorvalue(x): # sum = 0. # for i in x: # e = i.item() # sum =+ (e*e) # return math.sqrt(sum) # TCList = [] # TC = tensorvalue(tensor([3,15,12,42,37])) # for i,v in enumerate(Latency): # if (tensorvalue(v.cpu()) == TC ) : # TCList.append(i) # print(i) . 80 97 113 147 171 197 330 375 386 553 583 595 693 746 759 769 806 851 902 903 968 1001 1073 1210 1222 1239 1352 1377 1399 1414 1512 1538 1584 . Latency . tensor([[12, 0, 15, 19, 9], [ 0, 3, 35, 4, 28], [ 0, 2, 28, 31, 9], ..., [21, 15, 3, 33, 39], [10, 5, 41, 6, 21], [49, 36, 39, 33, 43]], device=&#39;cuda:0&#39;) . md=movies.iloc[idxm5s[:].cpu()].title . Choose the top &quot;Twenty&quot; movies with the highest weight for the first latent weight . top20movies = md.to_list()[0:Twenty] . print(TCList[0],movies.iloc[TCList[0]]) Twenty = len(TCList) top20movies = [md.to_list()[i] for i in TCList][0:20] . 23 movie 24 title Rumble in the Bronx (1995) Name: 23, dtype: object . top20movies[0] . &#39;Jade (1995)&#39; . Here we take the Latent Factors for the Movies and replicate to the Movie reviews from IMBD . LatencyString=[] for j in Latency[0:Twenty]: LT =&#39;&#39; temp_i = len(j) for k in j: MN = (k.item()) temp_i -= 1 if temp_i &gt; 0 : LT=LT+str(k.item())+&#39;;&#39; else: LT=LT+str(k.item()) LatencyString.append(LT) . LatencyString[0:10] . [&#39;12;0;15;19;9&#39;, &#39;0;3;35;4;28&#39;, &#39;0;2;28;31;9&#39;, &#39;0;8;28;13;45&#39;, &#39;0;3;46;35;8&#39;, &#39;0;3;15;39;36&#39;, &#39;35;0;7;8;10&#39;, &#39;31;15;0;40;13&#39;, &#39;0;15;29;12;20&#39;, &#39;0;9;8;45;14&#39;] . pdload=[] reviewCount = [] for i in range(len(top20movies)): ia = IMDb() imovies = ia.search_movie(str(top20movies[i])) if imovies != [] : mi = imovies[0].movieID imdbdata = ia.get_movie_reviews(str(mi))[&#39;data&#39;] # [&#39;reviews&#39;] if &#39;reviews&#39; in imdbdata: details = imdbdata[&#39;reviews&#39;] print(f&#39;*** {i} *** {len(details)} reviews&#39;) reviewCount.append((top20movies[i],len(details))) for j in details: #print(j[&#39;content&#39;]) #print(&#39;&#39;) pdload.append([top20movies[i],LatencyString[i],j[&#39;content&#39;]]) . *** 0 *** 24 reviews *** 1 *** 2 reviews *** 2 *** 11 reviews *** 3 *** 25 reviews *** 4 *** 19 reviews *** 5 *** 25 reviews *** 6 *** 25 reviews *** 8 *** 24 reviews *** 9 *** 24 reviews *** 10 *** 25 reviews *** 12 *** 25 reviews *** 13 *** 25 reviews *** 14 *** 25 reviews *** 15 *** 15 reviews *** 16 *** 21 reviews *** 17 *** 25 reviews *** 18 *** 25 reviews *** 19 *** 25 reviews . pdload[0] . [&#39;Jade (1995)&#39;, &#39;12;0;15;19;9&#39;, &#39;To say Friedkin &#39;s career has had its ups and downs is an understatement, his eighties filmography inarguably has enough bombs to sink a oil tanker. Yet eschewing their performances at the box office, many of his films yearn to be rediscovered, from &#34;Cruising&#34; to &#34;Deal of the Century&#34; to &#34;Rampage&#34;. Let &#39;s not kid ourselves, &#34;Jade&#34; is not a great film, and this is the fault of one man and one man alone - Joe Esterhas. If trash had a messiah, it would be him. For a fleeting moment in the nineties, Esterhas was paid by the bucketload to write formulaic movies for guys, and the erotic thriller has him to thank for its continuing lugubrious existence. &#34;Jade&#34; is interesting however, it is an erotic thriller without the erotic part. While Paul Verhoeven filled &#34;Basic Instinct&#34; chock full of the sleaze he had become renowned for, Friedkin &#39;s films are notable for primarily dealing with male characters, and are subsequently about as erotic as as a bowl of cereal. &#34;Jade&#34; is not about sex; it is about sexual jealousy. The talent of Linda Fiorentino cannot be underestimated here, giving depth to a part that amounts to no more than a typical male fantasy - part good girl, part whore - that &#39;s right, it &#39;s &#34;Crimes of Passion&#34; without Anthony Perkins and his bag of dildos. The leads are well cast and all give adequate performances, and Friedkin throws in all his usual directorial touches (subliminal images and, you guessed it, yet another bloody car chase). &#34;Jade&#34; is an enjoyable film, with delightfully silly twists and over-the-top violence (come on, you know you want to see Angie Everhart get run over again), and is given some class from it &#39;s cast and director, but, in the end, proves itself to be a guilty pleasure that makes one feel more guilt than pleasure.&#39;] . dfr=pd.DataFrame(pdload,columns=[&#39;Movie&#39;,&#39;Latency&#39;,&#39;Review&#39;]) . dfr.iloc[0] . Movie Jade (1995) Latency 12;0;15;19;9 Review To say Friedkin&#39;s career has had its ups and downs is an understatement, his eighties filmography inarguably has enough bombs to sink a oil tanker. Yet eschewing their performances at the box office, many of his films yearn to be rediscovered, from &#34;Cruising&#34; to &#34;Deal of the Century&#34; to &#34;Rampage&#34;. Let&#39;s not kid ourselves, &#34;Jade&#34; is not a great film, and this is the fault of one man and one man alone - Joe Esterhas. If trash had a messiah, it would be him. For a fleeting moment in the nineties, Esterhas was paid by the bucketload to write formulaic movies for guys, and the erotic thriller h... Name: 0, dtype: object . Now create the classifier but use vocab=dls_lm.vocab which we loaded from the Wiki IMDB dataloader . from fastai.text.all import * dlsr = TextDataLoaders.from_df(df=dfr, text_vocab=dls_lm.vocab,text_col=&#39;Review&#39;, label_col=&#39;Latency&#39;, label_delim=&quot;;&quot;,y_block=MultiCategoryBlock,splitter=RandomSplitter(0.2) ) dlsr.show_batch(max_n=3) dlsr.vocab[1] . /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return array(a, dtype, copy=False, order=order) . text None . 0 xxbos xxmaj the xxmaj great xxmaj famous as well as the xxmaj great xxmaj infamous occurrences in a nation &#39;s history are and always will be looked upon as subjects worthy of a film treatment . xxmaj whether an incident were to be filmed from a story and its screenplay form written directly for the screen or an adaptation of a xxmaj novel , a xxmaj short xxmaj story or a xxmaj stage xxmaj play , they are an important element of formulating our image of our xxmaj world . xxmaj right or wrong , it is a fact of life and one that we must learn to live xxunk good example of just what we are driving at is the occurrences of xxmaj october 26 , 1881 in xxmaj tombstone , xxmaj arizona . xxmaj known as &quot; the xxmaj gunfight at xxup o.k . xxmaj corral &quot; , | 0;16;27;34;9 | . 1 xxbos i often hear similar stories of people &#39;s first experiences watching xxmaj blade xxmaj runner , finding the film dull but coming to appreciate it years later - my story has the same trajectory . i first tried to watch xxmaj blade xxmaj runner ( of what i believe was xxmaj the xxmaj final xxmaj cut ) on xxup tv in xxmaj christmas 2009 , only to stop watching after half an hour due to boredom . xxmaj over the years , however , i would be compelled to return to xxmaj blade xxmaj runner several times and get more out of it with each viewing . xxmaj the tech - noir world of xxmaj blade xxmaj runner is one to get lost in with its use of neon and many billboards of geishas , albeit a more depressing , dystopian one than say that of xxmaj star xxmaj | 0;13;28;45;8 | . 2 xxbos xxmaj who could have possibly thought that xxmaj oliver xxmaj stone would surpass the level reached by &quot; jfk &quot; , one of the greatest political thrillers ever made ? xxmaj yet his &quot; nixon &quot; is not only the harrowing political biography of the most controversial and unpopular xxmaj president of the xxmaj united xxmaj states , but also the gripping psychological study of a tormented man who got too much to prove . xxmaj it &#39;s also a terrific movie served by an impressive casting : xxmaj anthony xxmaj hopkins , xxmaj joan xxmaj allen , xxmaj james xxmaj woods , xxmaj bob xxmaj hoskins , xxmaj ed xxmaj harris , xxup e.g. xxmaj marshall , xxmaj madeline xxmaj kahn and an unrecognizable xxmaj paul xxmaj sorvino as xxmaj henry xxmaj kissinger . xxmaj and it &#39;s definitely one of the best movies of 1995 , which | 0;28;36;4;9 | . [&#39;0&#39;, &#39;10&#39;, &#39;11&#39;, &#39;12&#39;, &#39;13&#39;, &#39;14&#39;, &#39;15&#39;, &#39;16&#39;, &#39;17&#39;, &#39;19&#39;, &#39;2&#39;, &#39;20&#39;, &#39;24&#39;, &#39;25&#39;, &#39;27&#39;, &#39;28&#39;, &#39;29&#39;, &#39;3&#39;, &#39;31&#39;, &#39;34&#39;, &#39;35&#39;, &#39;36&#39;, &#39;38&#39;, &#39;39&#39;, &#39;4&#39;, &#39;40&#39;, &#39;42&#39;, &#39;45&#39;, &#39;46&#39;, &#39;47&#39;, &#39;49&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;] . Using our Wiki/IMDB language model encoder let us build a classifier in order to predict the latent factors from the reviews. So we are building a classifier which is looking for the number of latent factors which were present in the top &quot;Twenty&quot; films. We are using the pre-trained Wiki-IMDB encoder and the IMDB vocab used in the pre-training. . learnr = text_classifier_learner(dlsr, AWD_LSTM, drop_mult=0.5, n_out=len(dlsr.vocab[1]), metrics=[]).to_fp16() . (len(dlsr.vocab[1]),len(counter),len(dls_lm.vocab),len(dlsr.vocab[0]),len(dlsr.vocab[1])) #dls_lm.vocab . (34, 50, 60008, 60008, 34) . learnr.load_encoder(&#39;finetunedE&#39;) . &lt;fastai.text.learner.TextLearner at 0x7f6340e8d210&gt; . . learnr.fit_one_cycle(1,2e-2) . epoch train_loss valid_loss time . 0 | 0.713931 | 0.663639 | 00:02 | . Following the box and slowly unfreezing . learn.freeze_to(-2) learnr.fit_one_cycle(1,slice(1e-2/(2.6**4),1e-2)) learn.freeze_to(-3) learnr.fit_one_cycle(1,slice(5e-3/(2.6**4),5e-3)) learnr.unfreeze() learnr.fit_one_cycle(2,slice(1e-3/(2.6**4),1e-3)) . epoch train_loss valid_loss time . 0 | 0.655238 | 0.648486 | 00:02 | . epoch train_loss valid_loss time . 0 | 0.632271 | 0.661978 | 00:02 | . epoch train_loss valid_loss time . 0 | 0.625277 | 0.677263 | 00:04 | . 1 | 0.619720 | 0.683465 | 00:04 | . dfr.iloc[0,2] . &#39;To say Friedkin &#39;s career has had its ups and downs is an understatement, his eighties filmography inarguably has enough bombs to sink a oil tanker. Yet eschewing their performances at the box office, many of his films yearn to be rediscovered, from &#34;Cruising&#34; to &#34;Deal of the Century&#34; to &#34;Rampage&#34;. Let &#39;s not kid ourselves, &#34;Jade&#34; is not a great film, and this is the fault of one man and one man alone - Joe Esterhas. If trash had a messiah, it would be him. For a fleeting moment in the nineties, Esterhas was paid by the bucketload to write formulaic movies for guys, and the erotic thriller has him to thank for its continuing lugubrious existence. &#34;Jade&#34; is interesting however, it is an erotic thriller without the erotic part. While Paul Verhoeven filled &#34;Basic Instinct&#34; chock full of the sleaze he had become renowned for, Friedkin &#39;s films are notable for primarily dealing with male characters, and are subsequently about as erotic as as a bowl of cereal. &#34;Jade&#34; is not about sex; it is about sexual jealousy. The talent of Linda Fiorentino cannot be underestimated here, giving depth to a part that amounts to no more than a typical male fantasy - part good girl, part whore - that &#39;s right, it &#39;s &#34;Crimes of Passion&#34; without Anthony Perkins and his bag of dildos. The leads are well cast and all give adequate performances, and Friedkin throws in all his usual directorial touches (subliminal images and, you guessed it, yet another bloody car chase). &#34;Jade&#34; is an enjoyable film, with delightfully silly twists and over-the-top violence (come on, you know you want to see Angie Everhart get run over again), and is given some class from it &#39;s cast and director, but, in the end, proves itself to be a guilty pleasure that makes one feel more guilt than pleasure.&#39; . learnr.predict(dfr.iloc[0,2]) . ((#13) [&#39;0&#39;,&#39;12&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;19&#39;,&#39;2&#39;,&#39;27&#39;,&#39;31&#39;...], tensor([ True, False, False, True, False, True, True, True, True, True, True, False, False, False, True, False, False, False, True, True, False, False, False, False, False, True, False, False, False, False, False, False, False, True]), tensor([0.5525, 0.4047, 0.4269, 0.6026, 0.4509, 0.5422, 0.6147, 0.5337, 0.6194, 0.5705, 0.5010, 0.4811, 0.4085, 0.4697, 0.5276, 0.4015, 0.4815, 0.4198, 0.5133, 0.5024, 0.4524, 0.4466, 0.4367, 0.4472, 0.4352, 0.5325, 0.4537, 0.4863, 0.4394, 0.4571, 0.4811, 0.3731, 0.3053, 0.5401])) . Now predict the latent factors for each movie based on their reviews . superpredictions=[] accRC=0 for m in range(len(reviewCount)): RC = reviewCount[m][1] Film = reviewCount[m][0] predictions=[] print(f&#39;film {Film} reviews {RC}&#39;) for l in range(reviewCount[m][1]+0): predictions.append(learnr.predict(dfr.iloc[accRC+l,2])[0]) accRC=accRC+RC superpredictions.append((Film,predictions)) . film Jade (1995) reviews 24 . film Paris Was a Woman (1995) reviews 2 . film Princess Bride, The (1987) reviews 11 . film Blade Runner (1982) reviews 25 . film Letter From Death Row, A (1998) reviews 19 . film Ice Storm, The (1997) reviews 25 . film Paradise Lost: The Child Murders at Robin Hood Hills (1996) reviews 25 . film Ulee&#39;s Gold (1997) reviews 24 . film C&#39;est arrivé près de chez vous (1992) reviews 24 . film Pretty Woman (1990) reviews 25 . film Nixon (1995) reviews 25 . film Browning Version, The (1994) reviews 25 . film Young Frankenstein (1974) reviews 25 . film Naked in New York (1994) reviews 15 . film Bewegte Mann, Der (1994) reviews 21 . film If Lucy Fell (1996) reviews 25 . film Some Like It Hot (1959) reviews 25 . film Twilight (1998) reviews 25 . (superpredictions[0][0],superpredictions[0][1]) . (&#39;Jade (1995)&#39;, [(#13) [&#39;0&#39;,&#39;12&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;19&#39;,&#39;2&#39;,&#39;27&#39;,&#39;31&#39;...], (#11) [&#39;0&#39;,&#39;12&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;19&#39;,&#39;20&#39;,&#39;27&#39;,&#39;29&#39;,&#39;34&#39;...], (#21) [&#39;0&#39;,&#39;12&#39;,&#39;13&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;19&#39;,&#39;2&#39;,&#39;25&#39;...], (#15) [&#39;0&#39;,&#39;11&#39;,&#39;12&#39;,&#39;13&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;20&#39;,&#39;27&#39;...], (#13) [&#39;0&#39;,&#39;12&#39;,&#39;13&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;27&#39;,&#39;29&#39;,&#39;34&#39;...], (#12) [&#39;0&#39;,&#39;12&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;19&#39;,&#39;20&#39;,&#39;27&#39;,&#39;38&#39;,&#39;40&#39;...], (#16) [&#39;0&#39;,&#39;12&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;19&#39;,&#39;2&#39;,&#39;20&#39;,&#39;25&#39;...], (#12) [&#39;0&#39;,&#39;12&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;19&#39;,&#39;2&#39;,&#39;27&#39;,&#39;31&#39;,&#39;34&#39;...], (#13) [&#39;0&#39;,&#39;12&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;20&#39;,&#39;25&#39;,&#39;27&#39;,&#39;29&#39;,&#39;34&#39;...], (#11) [&#39;0&#39;,&#39;12&#39;,&#39;15&#39;,&#39;17&#39;,&#39;19&#39;,&#39;20&#39;,&#39;25&#39;,&#39;29&#39;,&#39;34&#39;,&#39;40&#39;...], (#9) [&#39;0&#39;,&#39;12&#39;,&#39;14&#39;,&#39;15&#39;,&#39;17&#39;,&#39;19&#39;,&#39;27&#39;,&#39;31&#39;,&#39;34&#39;], (#21) [&#39;0&#39;,&#39;12&#39;,&#39;13&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;2&#39;,&#39;20&#39;,&#39;25&#39;...], (#20) [&#39;0&#39;,&#39;11&#39;,&#39;12&#39;,&#39;13&#39;,&#39;16&#39;,&#39;17&#39;,&#39;2&#39;,&#39;24&#39;,&#39;27&#39;,&#39;28&#39;...], (#10) [&#39;0&#39;,&#39;12&#39;,&#39;13&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;19&#39;,&#39;27&#39;,&#39;29&#39;,&#39;34&#39;], (#13) [&#39;12&#39;,&#39;13&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;19&#39;,&#39;20&#39;,&#39;24&#39;,&#39;27&#39;,&#39;29&#39;...], (#11) [&#39;0&#39;,&#39;12&#39;,&#39;15&#39;,&#39;17&#39;,&#39;19&#39;,&#39;20&#39;,&#39;29&#39;,&#39;34&#39;,&#39;40&#39;,&#39;49&#39;...], (#7) [&#39;0&#39;,&#39;12&#39;,&#39;15&#39;,&#39;19&#39;,&#39;25&#39;,&#39;34&#39;,&#39;40&#39;], (#16) [&#39;0&#39;,&#39;12&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;19&#39;,&#39;2&#39;,&#39;20&#39;,&#39;27&#39;...], (#21) [&#39;0&#39;,&#39;12&#39;,&#39;13&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;2&#39;,&#39;20&#39;,&#39;25&#39;,&#39;28&#39;...], (#11) [&#39;0&#39;,&#39;12&#39;,&#39;15&#39;,&#39;19&#39;,&#39;20&#39;,&#39;25&#39;,&#39;29&#39;,&#39;31&#39;,&#39;34&#39;,&#39;40&#39;...], (#24) [&#39;0&#39;,&#39;11&#39;,&#39;13&#39;,&#39;14&#39;,&#39;16&#39;,&#39;17&#39;,&#39;2&#39;,&#39;20&#39;,&#39;24&#39;,&#39;25&#39;...], (#12) [&#39;0&#39;,&#39;12&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;19&#39;,&#39;20&#39;,&#39;27&#39;,&#39;29&#39;...], (#17) [&#39;0&#39;,&#39;12&#39;,&#39;13&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;19&#39;,&#39;20&#39;,&#39;24&#39;,&#39;27&#39;...], (#10) [&#39;0&#39;,&#39;12&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;19&#39;,&#39;27&#39;,&#39;34&#39;,&#39;9&#39;]]) . Now we are going to plot the predicted Latent Factors per movie . import math int(math.sqrt(len(superpredictions))) . 4 . superpredictions[4] . (&#39;Letter From Death Row, A (1998)&#39;, [(#22) [&#39;0&#39;,&#39;11&#39;,&#39;12&#39;,&#39;13&#39;,&#39;16&#39;,&#39;17&#39;,&#39;20&#39;,&#39;24&#39;,&#39;27&#39;,&#39;28&#39;...], (#20) [&#39;0&#39;,&#39;11&#39;,&#39;12&#39;,&#39;13&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;24&#39;,&#39;27&#39;...], (#17) [&#39;11&#39;,&#39;13&#39;,&#39;16&#39;,&#39;17&#39;,&#39;24&#39;,&#39;25&#39;,&#39;27&#39;,&#39;35&#39;,&#39;38&#39;,&#39;39&#39;...], (#15) [&#39;0&#39;,&#39;11&#39;,&#39;13&#39;,&#39;14&#39;,&#39;16&#39;,&#39;17&#39;,&#39;24&#39;,&#39;27&#39;,&#39;3&#39;,&#39;34&#39;...], (#19) [&#39;0&#39;,&#39;11&#39;,&#39;13&#39;,&#39;14&#39;,&#39;16&#39;,&#39;17&#39;,&#39;2&#39;,&#39;24&#39;,&#39;27&#39;,&#39;28&#39;...], (#20) [&#39;11&#39;,&#39;13&#39;,&#39;16&#39;,&#39;17&#39;,&#39;24&#39;,&#39;25&#39;,&#39;27&#39;,&#39;28&#39;,&#39;29&#39;,&#39;34&#39;...], (#21) [&#39;0&#39;,&#39;12&#39;,&#39;13&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;2&#39;,&#39;25&#39;,&#39;27&#39;...], (#18) [&#39;0&#39;,&#39;13&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;2&#39;,&#39;27&#39;,&#39;28&#39;,&#39;3&#39;...], (#21) [&#39;0&#39;,&#39;11&#39;,&#39;13&#39;,&#39;16&#39;,&#39;17&#39;,&#39;20&#39;,&#39;24&#39;,&#39;25&#39;,&#39;27&#39;,&#39;3&#39;...], (#20) [&#39;0&#39;,&#39;11&#39;,&#39;13&#39;,&#39;16&#39;,&#39;17&#39;,&#39;2&#39;,&#39;20&#39;,&#39;28&#39;,&#39;29&#39;,&#39;3&#39;...], (#19) [&#39;11&#39;,&#39;13&#39;,&#39;16&#39;,&#39;17&#39;,&#39;20&#39;,&#39;24&#39;,&#39;25&#39;,&#39;27&#39;,&#39;34&#39;,&#39;35&#39;...], (#19) [&#39;0&#39;,&#39;11&#39;,&#39;13&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;24&#39;,&#39;25&#39;,&#39;27&#39;,&#39;3&#39;...], (#18) [&#39;11&#39;,&#39;16&#39;,&#39;20&#39;,&#39;24&#39;,&#39;25&#39;,&#39;27&#39;,&#39;28&#39;,&#39;34&#39;,&#39;35&#39;,&#39;36&#39;...], (#22) [&#39;0&#39;,&#39;11&#39;,&#39;13&#39;,&#39;16&#39;,&#39;17&#39;,&#39;2&#39;,&#39;24&#39;,&#39;25&#39;,&#39;27&#39;,&#39;28&#39;...], (#22) [&#39;11&#39;,&#39;12&#39;,&#39;13&#39;,&#39;15&#39;,&#39;16&#39;,&#39;17&#39;,&#39;2&#39;,&#39;24&#39;,&#39;25&#39;,&#39;27&#39;...], (#20) [&#39;0&#39;,&#39;13&#39;,&#39;16&#39;,&#39;17&#39;,&#39;24&#39;,&#39;25&#39;,&#39;28&#39;,&#39;3&#39;,&#39;34&#39;,&#39;35&#39;...], (#18) [&#39;0&#39;,&#39;11&#39;,&#39;13&#39;,&#39;16&#39;,&#39;24&#39;,&#39;25&#39;,&#39;27&#39;,&#39;28&#39;,&#39;29&#39;,&#39;3&#39;...], (#19) [&#39;0&#39;,&#39;11&#39;,&#39;13&#39;,&#39;16&#39;,&#39;17&#39;,&#39;24&#39;,&#39;25&#39;,&#39;27&#39;,&#39;28&#39;,&#39;29&#39;...], (#14) [&#39;0&#39;,&#39;12&#39;,&#39;13&#39;,&#39;14&#39;,&#39;15&#39;,&#39;16&#39;,&#39;27&#39;,&#39;29&#39;,&#39;3&#39;,&#39;31&#39;...]]) . reviewCount[0:10] . [(&#39;Jade (1995)&#39;, 24), (&#39;Paris Was a Woman (1995)&#39;, 2), (&#39;Princess Bride, The (1987)&#39;, 11), (&#39;Blade Runner (1982)&#39;, 25), (&#39;Letter From Death Row, A (1998)&#39;, 19), (&#39;Ice Storm, The (1997)&#39;, 25), (&#39;Paradise Lost: The Child Murders at Robin Hood Hills (1996)&#39;, 25), (&#34;Ulee&#39;s Gold (1997)&#34;, 24), (&#34;C&#39;est arrivé près de chez vous (1992)&#34;, 24), (&#39;Pretty Woman (1990)&#39;, 25)] . . From observation COLAB likes all the matplotlib in one cell . matplotlib.rcParams[&#39;figure.figsize&#39;] = [256, 64*1024/256-1] import math pict = int(math.sqrt(len(superpredictions)))+1 if pict &gt; 8 : pict = 8 pictw = pict picth = ((len(superpredictions))//pict) + 1 print(picth,pictw) _,axs = plt.subplots(picth,pictw) #axs.ravel() for boxcount,sp in enumerate(superpredictions): #print(sp[0]) reviewCountForTitle = reviewCount[boxcount][1] cord = (boxcount//(pictw),boxcount-(boxcount//pictw)*pictw) #cord = boxcount flat_list = [item for sublist in sp[1] for item in sublist] flat_list.sort() axs[cord].hist(flat_list, density=False, bins=len(dlsr.vocab[1])) # density=False would make counts axs[cord].set_title(str(sp[0]+&quot;:&quot;+str(reviewCountForTitle)),fontsize=108) # axs[cord].ylabel(&#39;Frequency&#39;) # axs[cord].xlabel(&#39;Data&#39;); plt.tight_layout() . 4 5 .",
            "url": "https://conwyn.github.io/MyBlogs/fastai/language/ordinality/2021/05/21/_05_22_MLIMDB.html",
            "relUrl": "/fastai/language/ordinality/2021/05/21/_05_22_MLIMDB.html",
            "date": " • May 21, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Cyclic-GAN Horses and Zebras",
            "content": "!pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastai import * . |████████████████████████████████| 727kB 9.0MB/s |████████████████████████████████| 1.2MB 18.8MB/s |████████████████████████████████| 51kB 7.9MB/s |████████████████████████████████| 194kB 39.1MB/s |████████████████████████████████| 61kB 9.1MB/s Mounted at /content/gdrive . !ls -al /content/gdrive/MyDrive/fastai . total 12 drwx 2 root root 4096 Jan 13 21:30 conwyn drwx 2 root root 4096 Mar 13 2019 data drwx 2 root root 4096 Mar 13 2019 models . !mkdir /content/gdrive/MyDrive/fastai/conwyn . !rmdir /content/gdrive/MyDrive/fastai/conwyn . import fastai fastai.__version__ . &#39;2.2.5&#39; . !ls -al /content/gdrive/MyDrive/fastai . total 8 drwx 2 root root 4096 Mar 13 2019 data drwx 2 root root 4096 Mar 13 2019 models . from fastai.vision.all import * . . horses = DataBlock(blocks=(ImageBlock,CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2,seed=42), get_y=parent_label, item_tfms=Resize(128)) . dlshorses=horses.dataloaders(&#39;/content/gdrive/MyDrive/Colab Notebooks/horse2zebra/trainhorses&#39;) . dlshorses.valid.show_batch(max_n=4,nrows=1) . zebras = DataBlock(blocks=(ImageBlock,CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2,seed=42), get_y=parent_label, item_tfms=Resize(128)) . dlszebras=zebras.dataloaders(&#39;/content/gdrive/MyDrive/Colab Notebooks/horse2zebra/trainzebras&#39;) . dlszebras.valid.show_batch(max_n=4,nrows=1) . learnhorses = cnn_learner(dlshorses,resnet18,metrics=error_rate) . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . learnhorses.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 0.000000 | 0.000000 | 0.000000 | 02:47 | . epoch train_loss valid_loss error_rate time . 0 | 0.000000 | 0.000000 | 0.000000 | 00:05 | . 1 | 0.000000 | 0.000000 | 0.000000 | 00:05 | . 2 | 0.000000 | 0.000000 | 0.000000 | 00:05 | . 3 | 0.000000 | 0.000000 | 0.000000 | 00:05 | . dlshorses . &lt;fastai.data.core.DataLoaders at 0x7feef5dd3c18&gt; . t_img=dlshorses.one_batch()[0] print(t_img.shape) t_img[0].permute(2,1, 0).shape . (64, 3, 128, 128) . (128, 128, 3) . T_newimage=t_img[0].permute(2,1,0).shape T_newimage . (128, 128, 3) . plt.imshow(t_img[0].permute(1,2,0).cpu()) . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . &lt;matplotlib.image.AxesImage at 0x7feef4021ac8&gt; . horsex = (t_img[0].permute(1,2,0).cpu()) . horsex . TensorImage([[[ 0.2453, 0.4678, 0.5311], [ 0.0398, 0.2577, 0.2348], [ 0.0569, 0.2577, 0.1128], ..., [ 0.0912, 0.4328, 0.0082], [ 0.0741, 0.4153, 0.0082], [ 0.0741, 0.4153, 0.0082]], [[ 0.3652, 0.6078, 0.6705], [-0.2171, -0.0049, -0.0267], [-0.1657, 0.0476, -0.0964], ..., [ 0.2967, 0.6254, 0.2173], [ 0.2282, 0.5553, 0.1651], [ 0.2282, 0.5553, 0.1476]], [[ 0.5364, 0.7829, 0.8448], [-0.2684, -0.0399, -0.0615], [-0.1657, 0.0651, -0.0615], ..., [ 0.4851, 0.7829, 0.4091], [ 0.4851, 0.7829, 0.4091], [ 0.5193, 0.8354, 0.4614]], ..., [[ 0.0569, 0.2927, 0.0431], [-0.0287, 0.2052, -0.0441], [ 0.0056, 0.2227, -0.0267], ..., [-0.5424, -0.4951, -0.6193], [ 0.1939, 0.2577, 0.1476], [ 0.3652, 0.4328, 0.3219]], [[-0.1143, 0.1176, -0.0964], [-0.1486, 0.0826, -0.1312], [-0.0287, 0.1877, -0.0092], ..., [-0.4397, -0.4601, -0.5670], [ 0.2111, 0.2052, 0.1302], [ 0.2967, 0.2927, 0.1999]], [[-0.2856, -0.0749, -0.2532], [-0.2342, -0.0224, -0.2010], [-0.0801, 0.1352, -0.0441], ..., [-0.3541, -0.3901, -0.4798], [ 0.0056, -0.0574, -0.1312], [ 0.0569, -0.0224, -0.0790]]]) . import tensorflow as tf im8=tf.image.convert_image_dtype(horsex,tf.uint8) . jp8=tf.image.encode_jpeg(horsex) . %matplotlib inline . matplotlib.pyplot.imshow(im8) . &lt;matplotlib.image.AxesImage at 0x7feef40638d0&gt; . matplotlib.pyplot.imsave(&#39;horsex.jpeg&#39;,im8) . TypeError Traceback (most recent call last) &lt;ipython-input-39-f3c1fdedb274&gt; in &lt;module&gt;() -&gt; 1 matplotlib.pyplot.imsave(&#39;horsex.jpeg&#39;,im8) /usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py in imsave(fname, arr, **kwargs) 2064 @docstring.copy(matplotlib.image.imsave) 2065 def imsave(fname, arr, **kwargs): -&gt; 2066 return matplotlib.image.imsave(fname, arr, **kwargs) 2067 2068 /usr/local/lib/python3.6/dist-packages/matplotlib/image.py in imsave(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs) 1548 if origin == &#34;lower&#34;: 1549 arr = arr[::-1] -&gt; 1550 rgba = sm.to_rgba(arr, bytes=True) 1551 if format == &#34;png&#34; and pil_kwargs is None: 1552 with cbook.open_file_cm(fname, &#34;wb&#34;) as file: /usr/local/lib/python3.6/dist-packages/matplotlib/cm.py in to_rgba(self, x, alpha, bytes, norm) 215 alpha = np.uint8(alpha * 255) 216 m, n = x.shape[:2] --&gt; 217 xx = np.empty(shape=(m, n, 4), dtype=x.dtype) 218 xx[:, :, :3] = x 219 xx[:, :, 3] = alpha TypeError: Cannot interpret &#39;tf.uint8&#39; as a data type . tf.image.encode_jpeg(&#39;gdrive/MyDrive/Colab Notebooks/bird/horsexx.jpeg&#39;,jp8) print(imgs[0].shape) print(content.shape) . AttributeError Traceback (most recent call last) &lt;ipython-input-28-8e8b23eb71be&gt; in &lt;module&gt;() -&gt; 1 matplotlib.image.imsave(&#39;gdrive/MyDrive/Colab Notebooks/bird/horsexx.jpeg&#39;,jp8) 2 print(imgs[0].shape) 3 print(content.shape) /usr/local/lib/python3.6/dist-packages/matplotlib/image.py in imsave(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs) 1563 if pil_kwargs is None: 1564 pil_kwargs = {} -&gt; 1565 pil_shape = (rgba.shape[1], rgba.shape[0]) 1566 image = Image.frombuffer( 1567 &#34;RGBA&#34;, pil_shape, rgba, &#34;raw&#34;, &#34;RGBA&#34;, 0, 1) AttributeError: &#39;tuple&#39; object has no attribute &#39;shape&#39; .",
            "url": "https://conwyn.github.io/MyBlogs/fastai/cyclic-gan/2021/05/21/HorsesAndZebras.html",
            "relUrl": "/fastai/cyclic-gan/2021/05/21/HorsesAndZebras.html",
            "date": " • May 21, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://conwyn.github.io/MyBlogs/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://conwyn.github.io/MyBlogs/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://conwyn.github.io/MyBlogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://conwyn.github.io/MyBlogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}